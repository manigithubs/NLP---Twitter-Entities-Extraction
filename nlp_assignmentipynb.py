# -*- coding: utf-8 -*-
"""NLP Assignmentipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jxhjTFhCbmONiZl8pFWoHxWreocb8NkR

# Twitter Tweets Entities Extraction and Sentiment Analysis Problem Statement

**Two Problem Statement**

You are given a JSON file (tweets.json) that contains tweets (sentences) along with the name of
the author.

Objective 1: Get the most frequent entities from the tweets.

Objective 2: Find out the sentiment/polarity of each author towards each of the entities.

# Steps involved in solving the problem statement:

Approach For This solution .Which i used to solve the problems statement .

Steps:

Importing Raw Data set : The first step in solving the objectives is to analyze the raw data. This involves reviewing the data to get an understanding of what information is contained in the data and what format the data is in.

raw data to a dataframe: Raw data can be stored in many formats and may not be easy to work with. To make the data more usable, it is converted into a dataframe, which is a two-dimensional table of data with labeled rows and columns.

Defining functions for future uses: Functions are reusable blocks of code that can be called multiple times with different inputs. In this step, functions are defined for tasks that will be repeated throughout the data analysis process.

Translating tweets to English: If the tweets are in a different language, they must be translated into English to make the analysis process easier.

Data pre-processing (cleaning tweets): The raw data will likely contain irrelevant information and/or errors, so it must be cleaned to remove this information. This step includes cleaning the tweets by removing unwanted elements such as hashtags, URLs, emojis, etc.

Finding text with spaces: Spaces between words can affect the analysis process, so it is important to remove any extra spaces from the tweets.

Numbers in the tweets: Numbers can also impact the analysis process, so it is important to remove or replace numbers in the tweets if necessary.

URL links: URLs can add noise to the data and make the analysis process more complicated, so it is important to remove any URLs from the tweets.

Hashtags: Hashtags can also add noise to the data, so it is important to remove any hashtags from the tweets.

Emojis: Emojis can also impact the analysis process, so it is important to remove any emojis from the tweets.

Words less than 2 characters: Words with fewer than 2 characters can also add noise to the data, so it is important to remove these words from the tweets.

Extracting entities from the tweets with their frequency: This step involves identifying and extracting entities (such as people, organizations, locations, etc.) from the tweets and determining the frequency of each entity.

Data visualization: The data must be visualized in a way that makes it easier to understand and analyze. This step involves creating visualizations such as bar charts, line charts, etc. to help understand the data.

Solution of Objective 1: The solution to Objective 1 involves using the processed data and the methods described above to solve the first objective.

Author's sentiment analysis through tweets: This step involves analyzing the sentiment (positive, negative, or neutral) of the authors as expressed in their tweets.

Solution of Objective 2: The solution to Objective 2 involves using the processed data and the methods described above to solve the second objective.

# Importing required Library
"""

import pandas as pd     
import numpy as np      
import json
import re
from nltk.corpus import stopwords 
from nltk.util import ngrams 
from collections import Counter
import nltk
import matplotlib.pyplot as plt
import seaborn as sns
from googletrans import Translator
import nltk
nltk.download('averaged_perceptron_tagger')
import nltk
nltk.download('stopwords')
nltk.download('brown')
nltk.download('wordnet')
nltk.download('brown')
nltk.download('punkt')
!pip install googletrans==4.0.0-rc1

"""# Reading raw Data set"""

#Reading Jason file as a dictonary 
with open("/content/tweets.json") as jfile:

  df = json.load(jfile)

df

"""# Converting raw data into Pandas Dataframe"""

#Creating a pandas dataframe out of dictonary
temp='tweet_author'
lst1 = [val[temp] for key, val in df.items() if temp in val]

temp1='tweet_text'
lst2 = [val[temp1] for key, val in df.items() if temp1 in val]

#Dropping id as it is of no use
data = pd.DataFrame(list(zip(lst1, lst2)),
               columns =['tweet_author', 'tweet_text'])

#For use in objective 2 making a copy of data
data_copy=data.copy()

"""# Visualizing"""

data.shape

data

"""# Converting Tweets into English"""

#Function for translating the tweets into English
translator = Translator(service_urls=[
      'translate.google.com',
      'translate.google.co.kr',
    ])
data['tweet_text'].astype(str)
data = data.loc[data['tweet_text'] != '']

#Replacing '.' with '. ' for Data translation issue
data['tweet_text'] = data['tweet_text'].str.replace('.', '. ')

#Visualization of the raw tweets in the data frame
data

"""# Removing hashtags,https, and RT,punctuations"""

#removing hashtags (word starting with #)
data['clean_tweet'] = np.vectorize(remove_pattern)(data['tweet_text'], "#[\w]*")

#removing hashtags (word starting with https)
data['clean_tweet'] = np.vectorize(remove_pattern)(data['clean_tweet'], "https[\w]*")

#remove user, https, and RT
data['clean_tweet'] = np.vectorize(remove_pattern)(data['clean_tweet'], "https|RT|@[\w]*")

#remove punctuations
data['clean_tweet'] = data['clean_tweet'].str.replace("[^a-zA-Z#]", " ")

#lowering string
data['clean_tweet'] = data['clean_tweet'].str.lower()

#remove stop words
stop_words = set(stopwords.words('english')) 


# #remove words with len < 2
data['clean_tweet'] = data['clean_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))

"""# Clean tweets Columns"""

#Creating a list of clean tweets 
data['clean_tweet'] = [' '.join([w for w in x.lower().split() if w not in stop_words]) 
    for x in data['clean_tweet'].tolist()]

"""# Visualizing Clean Tweets and Original Tweets"""

data

"""# Entities Extraction and How i am approached .

**Noun Phrases Approach**

<>Extracting entities based on Noun Phrases is a common approach in NER because noun phrases typically contain the most important information in a sentence.

<>Nouns often refer to entities such as people, places, organizations, and things, which are the primary focus of NER. Noun phrases also provide a way to capture the context of the entity in the sentence, making it easier to determine the sentiment towards the entity.

<>Using only noun phrases for entity extraction helps to reduce errors and improve accuracy compared to other techniques, such as extracting entities based on individual words or phrases that may not accurately represent the entity in question.

<> Additionally, noun phrases are easy to identify in a sentence and can be processed efficiently, making them a good choice for entity extraction in NER systems.

# TextBlob Package Approach
"""

#Importing TextBlob for looking for noun_Phrases into the tweets 

#An example from the extraction of noun_phrases on the sentence from assignment example
from textblob import TextBlob
wiki = TextBlob('Pink Pearl Apples are tasty but Empire Apples are not.')
wiki.noun_phrases

#Applying over the clean tweets column to extract the entities

from textblob import TextBlob

def blob(text):
  return TextBlob(text).noun_phrases

Entities = data['clean_tweet'].apply(blob)
data["entities"]=Entities

#Dataframe Visualization
data

"""# Counting frequency of the tweets"""

#Combining words into one list and count the appearance of each entity.

l=data["entities"]

#Counting the most repeated entity
flatten = [item for sublist in l for item in sublist]
counts = Counter(flatten).most_common()

entity_df = pd.DataFrame.from_records(counts, columns=['entity', 'frequency'])
entity_df['entity']= entity_df['entity'].apply(lambda x: ''.join([w for w in x]))

#New dataframe of most entity and there occurence
entity_df.head(10)

"""# Data Visualization"""

sns.set(rc={"figure.figsize" :(30,10)})
df_new = df_new.nlargest(columns="frequency", n = 30) 
ax = sns.barplot(data=df_new,x='entity',y='frequency')
ax.tick_params(axis='x',rotation=90)
ax.set(ylabel = 'frequency')
plt.show()

"""# Objective 1 File"""

entity_df.to_csv("objective1.csv")

#Creating the copy of the dataframe for further use
data2=data.copy()
data2

"""# Import spacy Model"""

#Importing required library
import pandas as pd
import re
import plotly.express as px
import nltk
import spacy

nlp = spacy.load('en_core_web_sm')

#installing vaderSentiment for utilization of SentimentIntensityAnalyzer
!pip install vaderSentiment

#Importing the library
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from nltk.sentiment.util import *

#Sentiment Analysis
SIA = SentimentIntensityAnalyzer()

data2["clean_tweet"]= data2["entities"].astype(str)
# Applying Model, Variable Creation
data2['Polarity Score']=data2["entities"].apply(lambda x:SIA.polarity_scores(x)['compound'])
data2['Neutral Score']=data2["entities"].apply(lambda x:SIA.polarity_scores(x)['neu'])
data2['Negative Score']=data2["entities"].apply(lambda x:SIA.polarity_scores(x)['neg'])
data2['Positive Score']=data2["entities"].apply(lambda x:SIA.polarity_scores(x)['pos'])
# Converting 0 to 1 Decimal Score to a Categorical Variable
data2['overall polarity']=''
data2.loc[data2['Polarity Score']>0,'overall polarity']='Positive'
data2.loc[data2['Polarity Score']==0,'overall polarity']='Neutral'
data2.loc[data2['Polarity Score']<0,'overall polarity']='Negative'
data2[:5]
data2[:5]

# Copying data2 in data3 for changing the the format for submission and conversion in csv file.
df3=data2.copy()

df3

"""# Dropping the excess columns"""

df3=df3.drop(['tweet_text',	'clean_tweet',	'Polarity Score',	'Neutral Score',	'Negative Score',	'Positive Score'], axis=1)

df3['entities'] = [','.join([''.join(y) for y in x]) for x in data['entities']]

df3=df3[['entities','tweet_author','overall polarity']]

df3.rename(columns = {'entities':'entity','tweet_author':'author'}, inplace = True)

"""# Final Solution  """

df3

"""# objective 2 File"""

df3.to_csv("objective2.csv")

"""# Final Statement For Twitter Tweets Entities Extraction.

In conclusion, our analysis of the tweets showed that the sentiment of the authors was predominantly positive, with a few instances of negative sentiment. This suggests that overall, the authors had a positive view of the topic being discussed in their tweets. The findings of this analysis have important implications for understanding how social media can be used to gauge public opinion on a particular topic. Based on these results, we recommend further research in this area to explore how social media sentiment can be used to inform decision making and policy development. The significance of this study lies in its contribution to the growing body of research on the use of social media data for public opinion analysis.
"""